{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassificadorXGBoost.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "background_execution": "on",
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/gabrielvinicios02/ExperimentsMultivariate/blob/f-predictDiagrama/ClassificadorXGBoost.ipynb",
      "authorship_tag": "ABX9TyP3n+10iW89wzyOgO854TrV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielvinicios02/ExperimentsMultivariate/blob/f-predictDiagrama/ClassificadorXGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ3W9LiliPPY"
      },
      "source": [
        "# Séries Temporais\n",
        "\n",
        "\n",
        "<center>\n",
        "<table><tr><td> <a href=\"http://www.minds.eng.ufmg.br/\"><img src=\"https://github.com/petroniocandido/pyFTS/raw/master/img/minds_logo_medium.jpeg\" alt=\"MINDS - Machine Intelligence and Data Science Lab\" width=\"100\"/></a></td> \n",
        "  <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
        "<td><a href=\"https://pyfts.github.io/pyFTS/\"><img src=\"https://github.com/petroniocandido/pyFTS/raw/master/img/logo_medium.png\" alt=\"pyFTS - Fuzzy Time Series for Python\" width=\"100\"/></a></td>\n",
        "</tr>\n",
        "</table>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -U git+https://github.com/PYFTS/pyFTS\n",
        "#!pip3 install -U pyFTS\n",
        "!pip3 install SimpSOM\n",
        "!pip install dispy\n",
        "#!pip install matplotlib==3.1.3\n",
        "!pip install matplotlib==3.1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nWMbE2RO8Ga1",
        "outputId": "27935796-8083-468a-e556-d06b08daacb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/PYFTS/pyFTS\n",
            "  Cloning https://github.com/PYFTS/pyFTS to /tmp/pip-req-build-gzair62z\n",
            "  Running command git clone -q https://github.com/PYFTS/pyFTS /tmp/pip-req-build-gzair62z\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pyFTS==1.6) (3.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyFTS==1.6) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pyFTS==1.6) (1.3.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyFTS==1.6) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyFTS==1.6) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyFTS==1.6) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyFTS==1.6) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->pyFTS==1.6) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->pyFTS==1.6) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pyFTS==1.6) (2022.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: SimpSOM in /usr/local/lib/python3.7/dist-packages (2.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from SimpSOM) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from SimpSOM) (1.21.6)\n",
            "Collecting matplotlib>=3.3.3\n",
            "  Using cached matplotlib-3.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.3->SimpSOM) (7.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.3->SimpSOM) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.3->SimpSOM) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.3->SimpSOM) (4.34.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.3->SimpSOM) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.3->SimpSOM) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.3->SimpSOM) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.3.3->SimpSOM) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.3->SimpSOM) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2.post1->SimpSOM) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2.post1->SimpSOM) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2.post1->SimpSOM) (1.1.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.1.1\n",
            "    Uninstalling matplotlib-3.1.1:\n",
            "      Successfully uninstalled matplotlib-3.1.1\n",
            "Successfully installed matplotlib-3.5.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dispy in /usr/local/lib/python3.7/dist-packages (4.15.1)\n",
            "Requirement already satisfied: pycos>=4.12.1 in /usr/local/lib/python3.7/dist-packages (from dispy) (4.12.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting matplotlib==3.1.1\n",
            "  Using cached matplotlib-3.1.1-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.1.1) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.1) (1.15.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.5.2\n",
            "    Uninstalling matplotlib-3.5.2:\n",
            "      Successfully uninstalled matplotlib-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7X7YJiV3Ucu"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "import matplotlib.pyplot as plt\n",
        "from pyFTS.models import hofts\n",
        "from pyFTS.partitioners import Grid\n",
        "from pyFTS.benchmarks import Measures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyFTS.data import TAIEX, Malaysia, sunspots, artificial\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "\n",
        "datasets = {}\n",
        "\n",
        "dff = pd.read_excel('../content/drive/MyDrive/DadosPWFTS/DadosFalha.xlsx')\n",
        "dfn = pd.read_excel('../content/drive/MyDrive/DadosPWFTS/DadosNormais.xlsx')\n",
        "\n",
        "dados_treino_Falha = dff[0:2300] \n",
        "dados_teste_Falha = dff[1800:2300]\n",
        "\n",
        "dados_treino_Normal = dfn[0:1800] \n",
        "dados_teste_Normal = dfn[1500:1800]"
      ],
      "metadata": {
        "id": "HzuCNrKnL98T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dados_treino_Falha)"
      ],
      "metadata": {
        "id": "1hgOGIqqPpeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dados_treino_Normal)"
      ],
      "metadata": {
        "id": "tgN0lJBPqli4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dados_teste_Falha)"
      ],
      "metadata": {
        "id": "lUTknJhWqlSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dados_teste_Normal)"
      ],
      "metadata": {
        "id": "zH5XveUeqw3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analisando as Séries Temporais"
      ],
      "metadata": {
        "id": "bEFAv7gGH3O2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dados com Falhas\n",
        "\n",
        "from pyFTS.data import TAIEX, Malaysia, sunspots, artificial\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "\n",
        "datasets1 = {}\n",
        "\n",
        "datasets1['LIT'] = dados_treino_Falha.get('LIT_2012KS_0251_FilteredSignal')\n",
        "datasets1['SpeedActual'] = dados_treino_Falha.get('_AL_2012KS_06M1_SpeedActual')\n",
        "datasets1['BeltLoad'] = dados_treino_Falha.get('WIT_1085KS_2300_BeltLoad')\n",
        "datasets1['Rate'] = dados_treino_Falha.get('WIT_1085KS_2300_Rate')\n",
        "\n",
        "\n",
        "fig1, ax = plt.subplots(nrows=4, ncols=4, figsize=[20,10])\n",
        "\n",
        "for ct, (key, data) in enumerate(datasets1.items()): \n",
        "  _lags = np.arange(0,101)\n",
        "  _acf = acf(data,nlags=100)\n",
        "  _pacf = pacf(data,nlags=100)\n",
        "  \n",
        "  ax[0][ct].set_title(\"{}\".format(key))\n",
        "  ax[0][ct].plot(data)\n",
        "  \n",
        "  ax[1][ct].set_title(\"{} 100 intances\".format(key))\n",
        "  ax[1][ct].plot(data[:100])\n",
        "  \n",
        "  ax[2][ct].set_title(\"ACF {} - 100 lags\".format(key))\n",
        "  ax[2][ct].plot(_lags, _acf, \"o\")\n",
        "  ax[2][ct].vlines(_lags,[0],_acf)\n",
        "  ax[2][ct].axhline(y=0, color='black')\n",
        "  \n",
        "  ax[3][ct].set_title(\"PACF {} - 100 lags\".format(key))\n",
        "  ax[3][ct].plot(_lags, _pacf, \"o\")\n",
        "  ax[3][ct].vlines(_lags,[0],_pacf)\n",
        "  ax[3][ct].axhline(y=0, color='black')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "-2UBTAIDw1Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dados normais - sem falhas\n",
        "from pyFTS.data import TAIEX, Malaysia, sunspots, artificial\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "\n",
        "datasets2 = {}\n",
        "\n",
        "datasets2['LIT'] = dados_treino_Normal.get('LIT_2012KS_0251_FilteredSignal')\n",
        "datasets2['SpeedActual'] = dados_treino_Normal.get('_AL_2012KS_06M1_SpeedActual')\n",
        "datasets2['BeltLoad'] = dados_treino_Normal.get('WIT_1085KS_2300_BeltLoad')\n",
        "datasets2['Rate'] = dados_treino_Normal.get('WIT_1085KS_2300_Rate')\n",
        "\n",
        "\n",
        "fig2, ax = plt.subplots(nrows=4, ncols=4, figsize=[20,10])\n",
        "\n",
        "for ct, (key, data) in enumerate(datasets2.items()): \n",
        "  _lags = np.arange(0,101)\n",
        "  _acf = acf(data,nlags=100)\n",
        "  _pacf = pacf(data,nlags=100)\n",
        "  \n",
        "  ax[0][ct].set_title(\"{}\".format(key))\n",
        "  ax[0][ct].plot(data)\n",
        "  \n",
        "  ax[1][ct].set_title(\"{} 100 intances\".format(key))\n",
        "  ax[1][ct].plot(data[:100])\n",
        "  \n",
        "  ax[2][ct].set_title(\"ACF {} - 100 lags\".format(key))\n",
        "  ax[2][ct].plot(_lags, _acf, \"o\")\n",
        "  ax[2][ct].vlines(_lags,[0],_acf)\n",
        "  ax[2][ct].axhline(y=0, color='black')\n",
        "  \n",
        "  ax[3][ct].set_title(\"PACF {} - 100 lags\".format(key))\n",
        "  ax[3][ct].plot(_lags, _pacf, \"o\")\n",
        "  ax[3][ct].vlines(_lags,[0],_pacf)\n",
        "  ax[3][ct].axhline(y=0, color='black')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "tbY2VNTXHhqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dados normais - sem falhas\n",
        "from pyFTS.data import TAIEX, Malaysia, sunspots, artificial\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "\n",
        "datasets3 = {}\n",
        "\n",
        "datasets3['LIT'] = dados_teste_Normal.get('LIT_2012KS_0251_FilteredSignal')\n",
        "datasets3['SpeedActual'] = dados_teste_Normal.get('_AL_2012KS_06M1_SpeedActual')\n",
        "datasets3['BeltLoad'] = dados_teste_Normal.get('WIT_1085KS_2300_BeltLoad')\n",
        "datasets3['Rate'] = dados_teste_Normal.get('WIT_1085KS_2300_Rate')\n",
        "\n",
        "\n",
        "fig3, ax = plt.subplots(nrows=4, ncols=4, figsize=[20,10])\n",
        "\n",
        "for ct, (key, data) in enumerate(datasets3.items()): \n",
        "  _lags = np.arange(0,101)\n",
        "  _acf = acf(data,nlags=100)\n",
        "  _pacf = pacf(data,nlags=100)\n",
        "  \n",
        "  ax[0][ct].set_title(\"{}\".format(key))\n",
        "  ax[0][ct].plot(data)\n",
        "  \n",
        "  ax[1][ct].set_title(\"{} 100 intances\".format(key))\n",
        "  ax[1][ct].plot(data[:100])\n",
        "  \n",
        "  ax[2][ct].set_title(\"ACF {} - 100 lags\".format(key))\n",
        "  ax[2][ct].plot(_lags, _acf, \"o\")\n",
        "  ax[2][ct].vlines(_lags,[0],_acf)\n",
        "  ax[2][ct].axhline(y=0, color='black')\n",
        "  \n",
        "  ax[3][ct].set_title(\"PACF {} - 100 lags\".format(key))\n",
        "  ax[3][ct].plot(_lags, _pacf, \"o\")\n",
        "  ax[3][ct].vlines(_lags,[0],_pacf)\n",
        "  ax[3][ct].axhline(y=0, color='black')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "FJRf4NTPZqvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyFTS.models import hofts\n",
        "from pyFTS.partitioners import Grid, FCM, CMeans, Entropy\n",
        "from pyFTS.benchmarks import Measures\n",
        "\n",
        "metodos = [Grid.GridPartitioner]#, Entropy.EntropyPartitioner, CMeans.CMeansPartitioner ]\n",
        "\n",
        "k = 30\n",
        "\n",
        "rows = []\n",
        "\n",
        "y1=np.asarray(datasets1['SpeedActual'])\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1,figsize=[15,5])\n",
        "\n",
        "ax.plot(y1, label='Original',color='black')\n",
        "\n",
        "for contador, metodo in enumerate(metodos):\n",
        "  part = metodo(data=y1, npart=k)\n",
        "  model = hofts.HighOrderFTS(order=2, partitioner=part)\n",
        "  model.fit(y1)\n",
        "  forecasts = model.predict(y1)\n",
        "  for o in range(model.order):\n",
        "    forecasts.insert(0,None)\n",
        "    \n",
        "  ax.plot(forecasts[:-1], label=part.name)\n",
        "  \n",
        "  rmse, mape, u = Measures.get_point_statistics(y1, model)\n",
        "  \n",
        "  rows.append([part.name, rmse, mape, u])\n",
        "  \n",
        "\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "lgd = ax.legend(handles, labels, loc=2, bbox_to_anchor=(1, 1))\n",
        "\n",
        "pd.DataFrame(rows, columns=['Partitions','RMSE','MAPE','U'])"
      ],
      "metadata": {
        "id": "WOLO39eiR-sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dados Sem Falhas\n",
        "from numpy.core.multiarray import concatenate\n",
        "from pyFTS.models import pwfts\n",
        "#from pyFTS.models import chen, cheng, hofts, pwfts\n",
        "from pyFTS.partitioners import Grid\n",
        "from pyFTS.benchmarks import Measures\n",
        "from pyFTS.common import Util\n",
        "\n",
        "#metodos = [chen.ConventionalFTS, cheng.TrendWeightedFTS, hofts.HighOrderFTS, \n",
        "#          hofts.WeightedHighOrderFTS, pwfts.ProbabilisticWeightedFTS]\n",
        "metodos = [pwfts.ProbabilisticWeightedFTS]\n",
        "\n",
        "k = 10\n",
        "y1 = datasets2['LIT']\n",
        "tam_grupo = 100\n",
        "i=0\n",
        "VT1=[]\n",
        "\n",
        "for i in range(0,y1.size,tam_grupo):\n",
        "  grpan = y1[i:i+tam_grupo]\n",
        "  part = Grid.GridPartitioner(data=np.asarray(grpan), npart=k)\n",
        "  for contador, metodo in enumerate(metodos):\n",
        "    model1 = metodo(partitioner=part, progress=False, type='distribution')\n",
        "    if model1.is_high_order:\n",
        "      model1 = metodo(partitioner=part, order=1)\n",
        "    model1.fit(np.asarray(y1))  \n",
        "  #print(model1)\n",
        "  _, _, lhs1, rhs1 = pwfts.highorder_fuzzy_markov_chain(model1)\n",
        "  lhs1t = np.transpose(lhs1)\n",
        "  V1 = np.c_[rhs1,lhs1t]\n",
        "  if i == 0 :\n",
        "    VT1 = V1\n",
        "  else:\n",
        "    VT1 = np.c_[VT1,V1]\n",
        "\n",
        "  print(i)\n",
        "\n",
        "VT1 = np.transpose(VT1)\n",
        "print(VT1)"
      ],
      "metadata": {
        "id": "pcFNIwoBHyj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dados Sem Falhas\n",
        "from pyFTS.models import pwfts\n",
        "#from pyFTS.models import chen, cheng, hofts, pwfts\n",
        "from pyFTS.partitioners import Grid\n",
        "from pyFTS.benchmarks import Measures\n",
        "\n",
        "#metodos = [chen.ConventionalFTS, cheng.TrendWeightedFTS, hofts.HighOrderFTS, \n",
        "#          hofts.WeightedHighOrderFTS, pwfts.ProbabilisticWeightedFTS]\n",
        "metodos = [pwfts.ProbabilisticWeightedFTS]\n",
        "\n",
        "k = 10\n",
        "y2 = datasets2['SpeedActual']\n",
        "tam_grupo = 100\n",
        "i=0\n",
        "VT2=[]\n",
        "V2=[]\n",
        "\n",
        "for i in range(0,y2.size,tam_grupo):\n",
        "  grpan = y2[i:i+tam_grupo]\n",
        "  part = Grid.GridPartitioner(data=np.asarray(grpan), npart=k)\n",
        "  for contador, metodo in enumerate(metodos):\n",
        "    model2 = metodo(partitioner=part, progress=False, type='distribution')\n",
        "    if model2.is_high_order:\n",
        "      model2 = metodo(partitioner=part, order=1)\n",
        "    model2.fit(np.asarray(y2))  \n",
        "  #print(model1)\n",
        "  _, _, lhs2, rhs2 = pwfts.highorder_fuzzy_markov_chain(model2)\n",
        "  lhs2t = np.transpose(lhs2)\n",
        "  V2 = np.c_[rhs2,lhs2t]\n",
        "  if i == 0 :\n",
        "    VT2 = V2\n",
        "  else:\n",
        "    VT2 = np.c_[VT2,V2]\n",
        "\n",
        "  print(i)\n",
        "\n",
        "VT2 = np.transpose(VT2)\n",
        "print(VT2)\n",
        "\n"
      ],
      "metadata": {
        "id": "fOwtB4OdW0HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dados Sem Falhas\n",
        "from pyFTS.models import pwfts\n",
        "#from pyFTS.models import chen, cheng, hofts, pwfts\n",
        "from pyFTS.partitioners import Grid\n",
        "from pyFTS.benchmarks import Measures\n",
        "\n",
        "#metodos = [chen.ConventionalFTS, cheng.TrendWeightedFTS, hofts.HighOrderFTS, \n",
        "#          hofts.WeightedHighOrderFTS, pwfts.ProbabilisticWeightedFTS]\n",
        "metodos = [pwfts.ProbabilisticWeightedFTS]\n",
        "\n",
        "k = 10\n",
        "y3 = datasets2['BeltLoad']\n",
        "tam_grupo = 100\n",
        "i=0\n",
        "VT3=[]\n",
        "V3=[]\n",
        "\n",
        "for i in range(0,y3.size,tam_grupo):\n",
        "  grpan = y3[i:i+tam_grupo]\n",
        "  part = Grid.GridPartitioner(data=np.asarray(grpan), npart=k)\n",
        "  for contador, metodo in enumerate(metodos):\n",
        "    model3 = metodo(partitioner=part, progress=False, type='distribution')\n",
        "    if model3.is_high_order:\n",
        "      model3 = metodo(partitioner=part, order=1)\n",
        "    model3.fit(np.asarray(y3))  \n",
        "  #print(model1)\n",
        "  _, _, lhs3, rhs3 = pwfts.highorder_fuzzy_markov_chain(model3)\n",
        "  lhs3t = np.transpose(lhs3)\n",
        "  V3 = np.c_[rhs3,lhs3t]\n",
        "  if i == 0 :\n",
        "    VT3 = V3\n",
        "  else:\n",
        "    VT3 = np.c_[VT3,V3]\n",
        "\n",
        "  print(i)\n",
        "\n",
        "VT3 = np.transpose(VT3)\n",
        "print(VT3)"
      ],
      "metadata": {
        "id": "HgsaaAc1Ecy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dados Sem Falhas\n",
        "from pyFTS.models import pwfts\n",
        "#from pyFTS.models import chen, cheng, hofts, pwfts\n",
        "from pyFTS.partitioners import Grid\n",
        "from pyFTS.benchmarks import Measures\n",
        "\n",
        "#metodos = [chen.ConventionalFTS, cheng.TrendWeightedFTS, hofts.HighOrderFTS, \n",
        "#          hofts.WeightedHighOrderFTS, pwfts.ProbabilisticWeightedFTS]\n",
        "metodos = [pwfts.ProbabilisticWeightedFTS]\n",
        "\n",
        "k = 10\n",
        "y4 = datasets2['Rate']\n",
        "tam_grupo = 100\n",
        "i=0\n",
        "VT4=[]\n",
        "V4=[]\n",
        "\n",
        "for i in range(0,y4.size,tam_grupo):\n",
        "  grpan = y4[i:i+tam_grupo]\n",
        "  part = Grid.GridPartitioner(data=np.asarray(grpan), npart=k)\n",
        "  for contador, metodo in enumerate(metodos):\n",
        "    model4 = metodo(partitioner=part, progress=False, type='distribution')\n",
        "    if model4.is_high_order:\n",
        "      model4 = metodo(partitioner=part, order=1)\n",
        "    model4.fit(np.asarray(y4))  \n",
        "  #print(model1)\n",
        "  _, _, lhs4, rhs4 = pwfts.highorder_fuzzy_markov_chain(model4)\n",
        "  lhs4t = np.transpose(lhs4)\n",
        "  V4 = np.c_[rhs4,lhs4t]\n",
        "  if i == 0 :\n",
        "    VT4 = V4\n",
        "  else:\n",
        "    VT4 = np.c_[VT4,V4]\n",
        "\n",
        "  print(i)\n",
        "\n",
        "VT4 = np.transpose(VT4)\n",
        "print(VT4)"
      ],
      "metadata": {
        "id": "y39M8xhwEnJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dados Com Falhas\n",
        "from numpy.core.multiarray import concatenate\n",
        "from pyFTS.models import pwfts\n",
        "#from pyFTS.models import chen, cheng, hofts, pwfts\n",
        "from pyFTS.partitioners import Grid\n",
        "from pyFTS.benchmarks import Measures\n",
        "from pyFTS.common import Util\n",
        "\n",
        "#metodos = [chen.ConventionalFTS, cheng.TrendWeightedFTS, hofts.HighOrderFTS, \n",
        "#          hofts.WeightedHighOrderFTS, pwfts.ProbabilisticWeightedFTS]\n",
        "metodos = [pwfts.ProbabilisticWeightedFTS]\n",
        "\n",
        "k = 10\n",
        "y1 = datasets1['LIT']\n",
        "tam_grupo = 100\n",
        "i=0\n",
        "VU1=[]\n",
        "\n",
        "for i in range(0,y1.size,tam_grupo):\n",
        "  grpan = y1[i:i+tam_grupo]\n",
        "  part = Grid.GridPartitioner(data=np.asarray(grpan), npart=k)\n",
        "  for contador, metodo in enumerate(metodos):\n",
        "    model1 = metodo(partitioner=part, progress=False, type='distribution')\n",
        "    if model1.is_high_order:\n",
        "      model1 = metodo(partitioner=part, order=1)\n",
        "    model1.fit(np.asarray(y1))  \n",
        "  #print(model1)\n",
        "  _, _, lhs1, rhs1 = pwfts.highorder_fuzzy_markov_chain(model1)\n",
        "  lhs1t = np.transpose(lhs1)\n",
        "  V1 = np.c_[rhs1,lhs1t]\n",
        "  if i == 0 :\n",
        "    VU1 = V1\n",
        "  else:\n",
        "    VU1 = np.c_[VU1,V1]\n",
        "\n",
        "  print(i)\n",
        "\n",
        "VU1 = np.transpose(VU1)\n",
        "print(VU1)"
      ],
      "metadata": {
        "id": "Ss7osjRS5NHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dados Com Falhas\n",
        "from pyFTS.models import pwfts\n",
        "#from pyFTS.models import chen, cheng, hofts, pwfts\n",
        "from pyFTS.partitioners import Grid\n",
        "from pyFTS.benchmarks import Measures\n",
        "\n",
        "#metodos = [chen.ConventionalFTS, cheng.TrendWeightedFTS, hofts.HighOrderFTS, \n",
        "#          hofts.WeightedHighOrderFTS, pwfts.ProbabilisticWeightedFTS]\n",
        "metodos = [pwfts.ProbabilisticWeightedFTS]\n",
        "\n",
        "k = 10\n",
        "y2 = datasets1['SpeedActual']\n",
        "tam_grupo = 100\n",
        "i=0\n",
        "VU2=[]\n",
        "V2=[]\n",
        "\n",
        "for i in range(0,y2.size,tam_grupo):\n",
        "  grpan = y2[i:i+tam_grupo]\n",
        "  part = Grid.GridPartitioner(data=np.asarray(grpan), npart=k)\n",
        "  for contador, metodo in enumerate(metodos):\n",
        "    model2 = metodo(partitioner=part, progress=False, type='distribution')\n",
        "    if model2.is_high_order:\n",
        "      model2 = metodo(partitioner=part, order=1)\n",
        "    model2.fit(np.asarray(y2))  \n",
        "  #print(model1)\n",
        "  _, _, lhs2, rhs2 = pwfts.highorder_fuzzy_markov_chain(model2)\n",
        "  lhs2t = np.transpose(lhs2)\n",
        "  V2 = np.c_[rhs2,lhs2t]\n",
        "  if i == 0 :\n",
        "    VU2 = V2\n",
        "  else:\n",
        "    VU2 = np.c_[VU2,V2]\n",
        "\n",
        "  print(i)\n",
        "\n",
        "VU2 = np.transpose(VU2)\n",
        "print(VU2)"
      ],
      "metadata": {
        "id": "IqEnIldv5M95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dados Com Falhas\n",
        "from pyFTS.models import pwfts\n",
        "#from pyFTS.models import chen, cheng, hofts, pwfts\n",
        "from pyFTS.partitioners import Grid\n",
        "from pyFTS.benchmarks import Measures\n",
        "\n",
        "#metodos = [chen.ConventionalFTS, cheng.TrendWeightedFTS, hofts.HighOrderFTS, \n",
        "#          hofts.WeightedHighOrderFTS, pwfts.ProbabilisticWeightedFTS]\n",
        "metodos = [pwfts.ProbabilisticWeightedFTS]\n",
        "\n",
        "k = 10\n",
        "y3 = datasets1['BeltLoad']\n",
        "tam_grupo = 100\n",
        "i=0\n",
        "VU3=[]\n",
        "V3=[]\n",
        "\n",
        "for i in range(0,y3.size,tam_grupo):\n",
        "  grpan = y3[i:i+tam_grupo]\n",
        "  part = Grid.GridPartitioner(data=np.asarray(grpan), npart=k)\n",
        "  for contador, metodo in enumerate(metodos):\n",
        "    model3 = metodo(partitioner=part, progress=False, type='distribution')\n",
        "    if model3.is_high_order:\n",
        "      model3 = metodo(partitioner=part, order=1)\n",
        "    model3.fit(np.asarray(y3))  \n",
        "  #print(model1)\n",
        "  _, _, lhs3, rhs3 = pwfts.highorder_fuzzy_markov_chain(model3)\n",
        "  lhs3t = np.transpose(lhs3)\n",
        "  V3 = np.c_[rhs3,lhs3t]\n",
        "  if i == 0 :\n",
        "    VU3 = V3\n",
        "  else:\n",
        "    VU3 = np.c_[VU3,V3]\n",
        "\n",
        "  print(i)\n",
        "\n",
        "VU3 = np.transpose(VU3)\n",
        "print(VU3)"
      ],
      "metadata": {
        "id": "Q_WVh9sg5M0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dados Com Falhas\n",
        "from pyFTS.models import pwfts\n",
        "#from pyFTS.models import chen, cheng, hofts, pwfts\n",
        "from pyFTS.partitioners import Grid\n",
        "from pyFTS.benchmarks import Measures\n",
        "\n",
        "#metodos = [chen.ConventionalFTS, cheng.TrendWeightedFTS, hofts.HighOrderFTS, \n",
        "#          hofts.WeightedHighOrderFTS, pwfts.ProbabilisticWeightedFTS]\n",
        "metodos = [pwfts.ProbabilisticWeightedFTS]\n",
        "\n",
        "k = 10\n",
        "y4 = datasets1['Rate']\n",
        "tam_grupo = 100\n",
        "i=0\n",
        "VU4=[]\n",
        "V4=[]\n",
        "\n",
        "for i in range(0,y4.size,tam_grupo):\n",
        "  grpan = y4[i:i+tam_grupo]\n",
        "  part = Grid.GridPartitioner(data=np.asarray(grpan), npart=k)\n",
        "  for contador, metodo in enumerate(metodos):\n",
        "    model4 = metodo(partitioner=part, progress=False, type='distribution')\n",
        "    if model4.is_high_order:\n",
        "      model4 = metodo(partitioner=part, order=1)\n",
        "    model4.fit(np.asarray(y4))  \n",
        "  #print(model1)\n",
        "  _, _, lhs4, rhs4 = pwfts.highorder_fuzzy_markov_chain(model4)\n",
        "  lhs4t = np.transpose(lhs4)\n",
        "  V4 = np.c_[rhs4,lhs4t]\n",
        "  if i == 0 :\n",
        "    VU4 = V4\n",
        "  else:\n",
        "    VU4 = np.c_[VU4,V4]\n",
        "\n",
        "  print(i)\n",
        "\n",
        "VU4 = np.transpose(VU4)\n",
        "print(VU4)"
      ],
      "metadata": {
        "id": "BA2Y43qO5MlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EMrCbZ8m8K1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ytst1 = np.ones(198)\n",
        "ytst2 = np.zeros(253)\n",
        "ytst = np.concatenate((ytst1,ytst2))"
      ],
      "metadata": {
        "id": "sqGLGkjrsMec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unir todas as matrizes geradas\n",
        "BaseT1 = np.transpose(np.concatenate([np.transpose(VT1),np.transpose(VT2),np.transpose(VT3),np.transpose(VT4)]))\n",
        "BaseT2 = np.transpose(np.concatenate([np.transpose(VU1),np.transpose(VU2),np.transpose(VU3),np.transpose(VU4)]))\n",
        "BaseT3 = np.c_[np.transpose(BaseT1), np.transpose(BaseT2)]\n",
        "BaseT4 = np.c_[np.transpose(BaseT3),ytst]"
      ],
      "metadata": {
        "id": "BCiWnRNhHxeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZKlxlVknGgad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pwfts.visualize_distributions(model1)"
      ],
      "metadata": {
        "id": "4QHy28_mw0rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwfts.visualize_distributions(model2)"
      ],
      "metadata": {
        "id": "TVT0gXwsGJlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwfts.visualize_distributions(model3)"
      ],
      "metadata": {
        "id": "QL-07qhcGJdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwfts.visualize_distributions(model4)"
      ],
      "metadata": {
        "id": "qxAYrX16GJHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGzSbQvu3gaJ"
      },
      "source": [
        "#dataset = pd.read_excel('../content/sample_data/Base.xlsx')\n",
        "#x = dataset.iloc[:,:-1].values\n",
        "#y = dataset.iloc[:,-1].values\n",
        "x = BaseT4[:,:-1]\n",
        "y = BaseT4[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyow2jEvnCqf"
      },
      "source": [
        "import seaborn as sns\n",
        "#dataset2 = pd.read_excel('../content/sample_data/Base.xlsx')\n",
        "corr_df = dff.corr(method='pearson')\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_df, annot=True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "#dataset2 = pd.read_excel('../content/sample_data/Base.xlsx')\n",
        "corr_df = dfn.corr(method='pearson')\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_df, annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ummyjgT6aGfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFJJUNfM4m2S"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x , y, test_size = 0.2, random_state = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcmHVc7q5qxo"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "Classifier = XGBClassifier()\n",
        "Classifier.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X0CkzIzb4mq"
      },
      "source": [
        "Matriz de confusão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyAwjWoW6chI"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "ypred = Classifier.predict(x_test)\n",
        "cm = confusion_matrix(y_test, ypred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNXjLi_Vb2Dv"
      },
      "source": [
        "#Precisão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcaS8zcBb1Tf"
      },
      "source": [
        "accuracy_score(y_test, ypred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xwNE9aL7f2g"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "accuracies = cross_val_score(estimator = Classifier, X = x_train, y = y_train, cv = 2)\n",
        "\n",
        "print(accuracies.mean()*100)\n",
        "print(accuracies.std()*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predição com Classificação"
      ],
      "metadata": {
        "id": "L4mMDLzE5jgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Impórtando os novos arquivos para realizar a classificação dos mesmos, porem com horizonte predito. Representando predição\n",
        "#1- Importar arquivos\n",
        "#2- Realizar predição a 100 e a 200 passos a frente(Intervalos de tempo aceitáveis)\n",
        "#3- Obter pesos e regras da PWFTS\n",
        "#4- Aplicar no Classificador já treinado anteriormente.\n",
        "\n",
        "dPredicaoNormal = pd.read_excel('../content/drive/MyDrive/DadosPWFTS/PrevNormal.xlsx')\n",
        "dPredicaoFalha = pd.read_excel('../content/drive/MyDrive/DadosPWFTS/PrevFalha.xlsx')\n",
        "\n",
        "N200passos = 200\n",
        "N100passos = 100"
      ],
      "metadata": {
        "id": "BRtxqxvf5i-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dados normais - sem falhas\n",
        "from pyFTS.data import TAIEX, Malaysia, sunspots, artificial\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "\n",
        "datasets5 = {}\n",
        "\n",
        "datasets5['LIT'] = dPredicaoNormal.get('LIT')\n",
        "datasets5['SpeedActual'] = dPredicaoNormal.get('SpeedActual')\n",
        "datasets5['BeltLoad'] = dPredicaoNormal.get('BeltLoad')\n",
        "datasets5['Rate'] = dPredicaoNormal.get('Rate')\n",
        "datasets5['Status'] = dPredicaoNormal.get('Status')\n"
      ],
      "metadata": {
        "id": "upB8PZl5994W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tratando os dados de nível\n",
        "#Dados Com Falhas\n",
        "from numpy.core.multiarray import concatenate\n",
        "from pyFTS.models import pwfts\n",
        "#from pyFTS.models import chen, cheng, hofts, pwfts\n",
        "from pyFTS.partitioners import Grid\n",
        "from pyFTS.benchmarks import Measures\n",
        "from pyFTS.common import Util\n",
        "\n",
        "#metodos = [chen.ConventionalFTS, cheng.TrendWeightedFTS, hofts.HighOrderFTS, \n",
        "#          hofts.WeightedHighOrderFTS, pwfts.ProbabilisticWeightedFTS]\n",
        "metodos = [pwfts.ProbabilisticWeightedFTS]\n",
        "\n",
        "k = 35#testar com 5000 dados e testar ordem 3\n",
        "y1 = datasets5['LIT']\n",
        "tam_grupo = 3000 #N100passos\n",
        "#tam_grupo = N100passos\n",
        "i=0\n",
        "FoN = 0\n",
        "#Validar variáveis\n",
        "VU1=[]\n",
        "V1_Markov =[]\n",
        "V1T=[]\n",
        "Bv1=[]\n",
        "\n",
        "for i in range(0,y1.size,tam_grupo):\n",
        "  grpan = y1[i:i+tam_grupo]\n",
        "  part = Grid.GridPartitioner(data=np.asarray(grpan), npart=k)\n",
        "  for contador, metodo in enumerate(metodos):\n",
        "    model1 = metodo(partitioner=part, progress=False, type='distribution')\n",
        "    if model1.is_high_order:\n",
        "      model1 = metodo(partitioner=part, order=3)\n",
        "    model1.fit(np.asarray(grpan))\n",
        "    #print(model1)  \n",
        "    #Predicao\n",
        "    PredictLIT = model1.predict(np.array(grpan),type='point', method='heuristic', steps_ahead=tam_grupo)\n",
        "    PredV1 = PredictLIT\n",
        "    #PredV1T = pd.concat(np.transpose(y1),PredV1)\n",
        "    print(PredV1)#Vizualização dos valores previstos da variavael 1\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=[15,3])\n",
        "    ax.plot(np.array(grpan[:100]))\n",
        "    ax.plot(PredV1[:100])\n",
        "    #Com valores previstos Treinar novo Modelo\n",
        "    partPred = Grid.GridPartitioner(data=np.asarray(PredV1), npart=k)\n",
        "    model11 = pwfts.ProbabilisticWeightedFTS(partitioner=part, progress=False, type='distribution')\n",
        "    model11.fit(np.asarray(PredV1))#Treinando novo modelo com os valores previstos \n",
        "    #print(model11)\n",
        "  #Obtenção modelo de regras PWFTS\n",
        "  _, _, lhs11, rhs11 = pwfts.highorder_fuzzy_markov_chain(model11)\n",
        "  FoN=sum(grpan)\n",
        "  print(lhs11)\n",
        "  print(rhs11)\n",
        "  lhs11t = np.transpose(lhs11)\n",
        "  V1_Markov = np.c_[rhs11,lhs11t]\n",
        "  print(i)\n",
        "print(V1_Markov)"
      ],
      "metadata": {
        "id": "PgOVZrh28ylr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tratando os dados de nível\n",
        "#Dados Com Falhas\n",
        "from numpy.core.multiarray import concatenate\n",
        "from pyFTS.models import pwfts\n",
        "#from pyFTS.models import chen, cheng, hofts, pwfts\n",
        "from pyFTS.partitioners import Grid\n",
        "from pyFTS.benchmarks import Measures\n",
        "from pyFTS.common import Util\n",
        "\n",
        "#metodos = [chen.ConventionalFTS, cheng.TrendWeightedFTS, hofts.HighOrderFTS, \n",
        "#          hofts.WeightedHighOrderFTS, pwfts.ProbabilisticWeightedFTS]\n",
        "metodos = [pwfts.ProbabilisticWeightedFTS]\n",
        "\n",
        "k = 95\n",
        "y2 = datasets5['SpeedActual']\n",
        "tam_grupo = 3000 #N100passos\n",
        "#tam_grupo = N100passos\n",
        "i=0\n",
        "FoN = 0\n",
        "#Validar variáveis\n",
        "VU2=[]\n",
        "V2_Markov =[]\n",
        "V2T=[]\n",
        "\n",
        "for i in range(0,y2.size,tam_grupo):\n",
        "  grpan = y2[i:i+tam_grupo]\n",
        "  part = Grid.GridPartitioner(data=np.asarray(grpan), npart=k)\n",
        "  for contador, metodo in enumerate(metodos):\n",
        "    model2 = metodo(partitioner=part, progress=False, type='distribution')\n",
        "    if model2.is_high_order:\n",
        "      model2 = metodo(partitioner=part, order=3)\n",
        "    model2.fit(np.asarray(y2)) \n",
        "    print(model2) \n",
        "    #Predicao\n",
        "    PredictSpeed = model2.predict(np.array(grpan),type='point', method='heuristic', steps_ahead=tam_grupo)\n",
        "    PredV2 = PredictSpeed\n",
        "    print(PredV2)#Vizualização dos valores previstos da variavael \n",
        "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=[15,3])\n",
        "    ax.plot(np.array(grpan[:100]))\n",
        "    ax.plot(PredV2[:100])\n",
        "    #Com valores previstos Treinar novo Modelo\n",
        "    partPred = Grid.GridPartitioner(data=np.asarray(PredV2), npart=k)\n",
        "    model22 = pwfts.ProbabilisticWeightedFTS(partitioner=part, progress=False, type='distribution')\n",
        "    model22.fit(np.asarray(PredV2))#Treinando novo modelo com os valores previstos \n",
        "    print(model22)\n",
        "  #Obtenção modelo de regras PWFTS\n",
        "  _, _, lhs22, rhs22 = pwfts.highorder_fuzzy_markov_chain(model22)\n",
        "\n",
        "  FoN=sum(grpan)\n",
        "  lhs22t = np.transpose(lhs22)\n",
        "  V2_Markov = np.c_[rhs22,lhs22t]\n",
        "  print(i)\n",
        "print(V2_Markov)"
      ],
      "metadata": {
        "id": "vriWoxGG-g85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwfts.visualize_distributions(model22)"
      ],
      "metadata": {
        "id": "auIK0EShV3HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tratando os dados de nível\n",
        "#Dados Com Falhas\n",
        "from numpy.core.multiarray import concatenate\n",
        "from pyFTS.models import pwfts\n",
        "#from pyFTS.models import chen, cheng, hofts, pwfts\n",
        "from pyFTS.partitioners import Grid\n",
        "from pyFTS.benchmarks import Measures\n",
        "from pyFTS.common import Util\n",
        "\n",
        "#metodos = [chen.ConventionalFTS, cheng.TrendWeightedFTS, hofts.HighOrderFTS, \n",
        "#          hofts.WeightedHighOrderFTS, pwfts.ProbabilisticWeightedFTS]\n",
        "metodos = [pwfts.ProbabilisticWeightedFTS]\n",
        "\n",
        "k = 35\n",
        "y3 = datasets5['BeltLoad']\n",
        "tam_grupo = 3000 #N100passos\n",
        "#tam_grupo = N100passos\n",
        "i=0\n",
        "\n",
        "#Validar variáveis\n",
        "VU3=[]\n",
        "V3_Markov =[]\n",
        "V3T=[]\n",
        "\n",
        "for i in range(0,y3.size,tam_grupo):\n",
        "  grpan = y3[i:i+tam_grupo]#1000 amostras\n",
        "  part = Grid.GridPartitioner(data=np.asarray(grpan), npart=k)\n",
        "  for contador, metodo in enumerate(metodos):\n",
        "    model3 = metodo(partitioner=part, progress=False, type='distribution')\n",
        "    if model3.is_high_order:\n",
        "      model3 = metodo(partitioner=part, order=2)\n",
        "    model3.fit(np.asarray(grpan))\n",
        "    print(model3)  \n",
        "    #Predicao\n",
        "    PredictBelt = model3.predict(np.array(grpan), type='point', method='heuristic', steps_ahead=tam_grupo)    \n",
        "    PredV3 = PredictBelt\n",
        "    print(PredV3)#Vizualização dos valores previstos da variavael 2\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=[15,3])\n",
        "    ax.plot(np.array(grpan[:100]))\n",
        "    ax.plot(PredV3[:100])\n",
        "    #Com valores previstos Treinar novo Modelo\n",
        "    partPred = Grid.GridPartitioner(data=np.asarray(PredV3), npart=k)\n",
        "    model33 = pwfts.ProbabilisticWeightedFTS(partitioner=part, progress=False, type='distribution')\n",
        "    model33.fit(np.asarray(PredV3))#Treinando novo modelo com os valores previstos \n",
        "    print(model33)\n",
        "    \n",
        "  #Obtenção modelo de regras PWFTS\n",
        "  _, _, lhs33, rhs33 = pwfts.highorder_fuzzy_markov_chain(model33)#Obtenção dos Pesos\n",
        "  _, _, lhs3, rhs3 = pwfts.highorder_fuzzy_markov_chain(model3)#Obtenção dos Pesos\n",
        "\n",
        "  FoN=sum(grpan)\n",
        "  lhs33t = np.transpose(lhs33)\n",
        "  V3_Markov = np.c_[rhs33,lhs33t]\n",
        "  print(i)\n",
        "print(V3_Markov)"
      ],
      "metadata": {
        "id": "TIMP3PgXDAxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(model3))"
      ],
      "metadata": {
        "id": "kwV1WISVAbrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwfts.visualize_distributions(model3)"
      ],
      "metadata": {
        "id": "UtGG77hPZwzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tratando os dados de nível\n",
        "#Dados Com Falhas\n",
        "from numpy.core.multiarray import concatenate\n",
        "from pyFTS.models import pwfts\n",
        "#from pyFTS.models import chen, cheng, hofts, pwfts\n",
        "from pyFTS.partitioners import Grid\n",
        "from pyFTS.benchmarks import Measures\n",
        "from pyFTS.common import Util\n",
        "\n",
        "#metodos = [chen.ConventionalFTS, cheng.TrendWeightedFTS, hofts.HighOrderFTS, \n",
        "#          hofts.WeightedHighOrderFTS, pwfts.ProbabilisticWeightedFTS]\n",
        "metodos = [pwfts.ProbabilisticWeightedFTS]\n",
        "\n",
        "k = 35\n",
        "y4 = datasets5['Rate']\n",
        "tam_grupo = 3000 #N100passos\n",
        "#tam_grupo = N100passos\n",
        "i=0\n",
        "FoN = 0 \n",
        "#Validar variáveis\n",
        "VU4=[]\n",
        "V4_Markov =[]\n",
        "V4T=[]\n",
        "\n",
        "for i in range(0,y4.size,tam_grupo):\n",
        "  grpan = y4[i:i+tam_grupo]\n",
        "  part = Grid.GridPartitioner(data=np.asarray(grpan), npart=k)\n",
        "  for contador, metodo in enumerate(metodos):\n",
        "    model4 = metodo(partitioner=part, progress=False, type='distribution')\n",
        "    if model4.is_high_order:\n",
        "      model4 = metodo(partitioner=part, order=3)\n",
        "    model4.fit(np.asarray(grpan))  \n",
        "    #Predicao\n",
        "    PredictRate = model4.predict(np.array(grpan),type='point', method='heuristic', steps_ahead=tam_grupo)\n",
        "    PredV4 = PredictRate\n",
        "    print(PredV4)#Vizualização dos valores previstos da variavael 1\n",
        "    #plt.plot(PredV4)\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=[15,3])\n",
        "    ax.plot(np.array(grpan[:100]))\n",
        "    ax.plot(PredV4[:100])\n",
        "      #Com valores previstos Treinar novo Modelo\n",
        "    partPred = Grid.GridPartitioner(data=np.asarray(PredV4), npart=k)\n",
        "    model44 = pwfts.ProbabilisticWeightedFTS(partitioner=part, progress=False, type='distribution')\n",
        "    model44.fit(np.asarray(PredV4))#Treinando novo modelo com os valores previstos \n",
        "  #Obtenção modelo de regras PWFTS\n",
        "  FOS, OS, lhs44, rhs44 = pwfts.highorder_fuzzy_markov_chain(model44)#Obtenção dos Pesos\n",
        "\n",
        "  FoN=sum(grpan)\n",
        "  lhs44t = np.transpose(lhs44)\n",
        "  V4_Markov = np.c_[rhs44,lhs44t]\n",
        "  print(i)\n",
        "print(V4_Markov)"
      ],
      "metadata": {
        "id": "vjIB64XKDAff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytst11 = np.ones(60)\n",
        "ytst22 = np.zeros(0)\n",
        "ytstt = np.concatenate((ytst11,ytst22))"
      ],
      "metadata": {
        "id": "zKZeZlG7Ygc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BaseTT1 = np.transpose(np.concatenate([np.transpose(V1_Markov),np.transpose(V2_Markov),np.transpose(V3_Markov),np.transpose(V4_Markov)]))\n",
        "BaseT4 = np.c_[np.transpose(BaseTT1),ytstt]"
      ],
      "metadata": {
        "id": "gfZ9RoJk7KJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = BaseT4[:,:-1]\n",
        "y = BaseT4[:,-1]"
      ],
      "metadata": {
        "id": "_7F9hNvpJWik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "id": "5cwy3SMbtJhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ypred)"
      ],
      "metadata": {
        "id": "U7aIgMrCtMeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "ypred = Classifier.predict(x)\n",
        "cm = confusion_matrix(y, ypred)\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "S9MshF74JWbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Análise de CCF"
      ],
      "metadata": {
        "id": "mK0xD-41ljdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GSspVUW8skh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3dhf-P8zskSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(nrows=4, ncols=2,figsize=[18,15])\n",
        "\n",
        "ax[0][0].set_title('ACF de LIT')\n",
        "plot_acf(datasets5['LIT'].values, 24, axis=ax[0][0])\n",
        "ax[0][1].set_title('ACF de SpeedActual')\n",
        "plot_acf(datasets5['SpeedActual'].values, 24, axis=ax[0][1])\n",
        "ax[1][0].set_title('ACF de BeltLoad')\n",
        "plot_acf(datasets5['BeltLoad'].values, 24, axis=ax[1][0])\n",
        "ax[1][1].set_title('ACF de Rate')\n",
        "plot_acf(datasets5['Rate'].values, 24, axis=ax[1][1])\n",
        "ax[2][0].set_title('CCF de LIT x LIT')\n",
        "plot_ccf(datasets5['LIT'].values, datasets5['LIT'].values, 24, axis=ax[2][0])\n",
        "ax[2][1].set_title('CCF de LIT x SpeedActual')\n",
        "plot_ccf(datasets5['LIT'].values, datasets5['SpeedActual'].values, 24, axis=ax[2][1])\n",
        "ax[3][0].set_title('CCF de LIT x BeltLoad')\n",
        "plot_ccf(datasets5['LIT'].values, datasets5['BeltLoad'].values, 24, axis=ax[3][0])\n",
        "ax[3][1].set_title('CCF de LIT x Rate')\n",
        "plot_ccf(datasets5['LIT'].values, datasets5['Rate'].values, 24, axis=ax[3][1])\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "eXKYh1QAmlCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from nsepy import get_history\n",
        "import datetime\n",
        "from datetime import date\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "#from nsepy.derivatives import get_expiry_date"
      ],
      "metadata": {
        "id": "OAv7wVyopyJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Var(X):\n",
        "  n = len(X)\n",
        "  mx = np.mean(X)\n",
        "  c = np.mean([(x - mx)**2 for x in X])\n",
        "  return c\n",
        "\n",
        "def AutoCov(X, k):\n",
        "  n = len(X)\n",
        "  mx = np.mean(X)\n",
        "  c = np.zeros(n-k)\n",
        "  for i in range(n - k):\n",
        "    c[i] = (X[i] - mx)*(X[i+k] - mx)\n",
        "  c = c.mean()\n",
        "  \n",
        "  return c\n",
        "\n",
        "def CrossCov(X, Y, k):\n",
        "  n = len(X)\n",
        "  mx = np.mean(X)\n",
        "  my = np.mean(Y)\n",
        "  c = np.zeros(n-k)\n",
        "  for i in range(n - k):\n",
        "    c[i] = (X[i] - mx)*(Y[i+k] - my)\n",
        "  c = c.mean()\n",
        "  \n",
        "  return c\n",
        "\n",
        "def ACF(X, k):\n",
        "  gamma_k = AutoCov(X, k)\n",
        "  gamma_0 = Var(X)\n",
        "  return gamma_k / gamma_0\n",
        "\n",
        "def CCF(X, Y, k):\n",
        "  gamma_xy = CrossCov(X, Y, k)\n",
        "  gamma_x = Var(X)\n",
        "  gamma_y = Var(Y)\n",
        "  return gamma_xy / np.sqrt(gamma_x * gamma_y)"
      ],
      "metadata": {
        "id": "gWDEPvIYNHJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_acf(X, k, **kwargs):\n",
        "  acf = []\n",
        "  for i in range(k):\n",
        "    acf.append(ACF(X, i))\n",
        "\n",
        "  ax = kwargs.get(\"axis\", None)\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=1,figsize=[10,5])\n",
        "\n",
        "  ax.hlines([0],[0],[k], color=\"black\")\n",
        "  ax.vlines([i for i in range(k)], [0 for i in range(k)], acf, color=\"red\")\n",
        "  ax.scatter([i for i in range(k)], acf, marker=\"*\")\n",
        "  ax.set_xlabel(\"k\")\n",
        "  ax.set_ylabel(\"ACF(k)\")\n",
        "\n",
        "def plot_ccf(X, Y, k, **kwargs):\n",
        "  ccf = []\n",
        "  for i in range(k):\n",
        "    ccf.append(CCF(X, Y,  i))\n",
        "\n",
        "  ax = kwargs.get(\"axis\", None)\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=1,figsize=[10,5])\n",
        "\n",
        "  ax.hlines([0],[0],[k], color=\"black\")\n",
        "  ax.vlines([i for i in range(k)], [0 for i in range(k)], ccf, color=\"red\")\n",
        "  ax.scatter([i for i in range(k)], ccf, marker=\"*\")\n",
        "  ax.set_xlabel(\"k\")\n",
        "  ax.set_ylabel(\"CCF(k)\")"
      ],
      "metadata": {
        "id": "-LJVA0SymeY7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}